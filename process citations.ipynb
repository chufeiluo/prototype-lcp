{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cda29e5a",
   "metadata": {},
   "source": [
    "## Process Citations\n",
    "\n",
    "Requirements: raw data from CourtListener, U.S. Codes from LII\n",
    "- parses all citations to the U.S. codes from every opinion document\n",
    "- automatically generates prediction labels of the top n citations (default=100)\n",
    "- splits data into train:dev:test with a ratio of 80:5:15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    "#!conda install -y -c huggingface transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a207672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "import datasets\n",
    "\n",
    "data_file = 'data/opinions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "uscode_reg = r'(?<=\\s)([\\d]+[a-z]*)\\s+U.S.C. §+ (([0-9]+[a-z]*[-–]*)+)(\\([a-z]\\))*(\\([\\d]\\))*'\n",
    "\n",
    "n = 100 # maximum label length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import preprocess\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce44f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# citations = pd.read_json('../citations.json')\n",
    "jurisdictions = []\n",
    "filenames = []\n",
    "text = []\n",
    "\n",
    "for root, dirs, files in os.walk(data_file):\n",
    "    for file in files:\n",
    "        jurisdictions.append(root.split('/')[-1])\n",
    "        filenames.append(file)\n",
    "        with open(root+file, 'r') as f:\n",
    "            temp = json.loads(''.join(f.readlines()))\n",
    "            text.append(temp['html_lawbox'])\n",
    "\n",
    "citations = {'jurisdiction': jurisdictions, 'file': filenames, 'text': text}\n",
    "\n",
    "citations = pd.DataFrame.from_dict(citations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb03360",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [x.split('/')[2].split('.')[0] for x in citations['file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test), len(set(test)), len(set(citations['file']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9de6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('opinions{0}'.format(citations['file'].iloc[1]), 'r', encoding='utf-8') as f:\n",
    "    test = json.loads(''.join([x.strip() for x in f.readlines()]))\n",
    "    \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03335498",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = citations[['title','section', 'subsection']].value_counts()[:100] # only predict top 100 most frequently cited US codes\n",
    "to_predict = ['_'.join(x) for x in list(to_predict.index)]\n",
    "\n",
    "\n",
    "label_dict = {k: v for v, k in enumerate(to_predict)}\n",
    "print(label_dict)\n",
    "\n",
    "citations['partial_citation'] = citations['title'] +'_'+ citations['section'] + '_' + citations['subsection']\n",
    "\n",
    "citations['file_name'] = ['{0}-{1}'.format(label_dict[x], x) if x in label_dict else None for x in citations['partial_citation'] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## reading in the relevant US codes\n",
    "\n",
    "lab_text = {}\n",
    "for k,v in label_dict.items():\n",
    "    paragraph = re.sub(r'[^a-z]', '', k[2])\n",
    "    if os.path.exists(f'lii/text/_uscode_text_{k[0]}_{k[1]}{paragraph}.txt'):\n",
    "        with open(f'lii/text/_uscode_text_{k[0]}_{k[1]}{paragraph}.txt') as f:\n",
    "            lab_text[k] = ' '.join([x.strip() for x in f.readlines()[1:]])\n",
    "    elif os.path.exists(f'lii/text/_uscode_text_{k[0]}_{k[1]}.txt'):\n",
    "        with open(f'lii/text/_uscode_text_{k[0]}_{k[1]}.txt') as f:\n",
    "            lab_text[k] = ' '.join([x.strip() for x in f.readlines()[1:]])\n",
    "    else:\n",
    "        print('U.S. Code not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f09abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('citation_map.json', 'w') as f:\n",
    "    f.write(json.dumps(label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in lab_text.items():\n",
    "    n = '_'.join(k)\n",
    "    with open(f'{label_dict[k]}-{n}.json', 'w') as f:\n",
    "        f.write(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations.to_json('citations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r_list = list(citations['file'].unique())\n",
    "print(len(r_list))\n",
    "\n",
    "citations_red = citations[citations['file'].isin(r_list)]\n",
    "len(citations_red)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682cc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations.head()\n",
    "citations['partial_citation'] = citations['partial_citation'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab418bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations['partial_citation'] = citations['partial_citation'].cat.rename_categories(label_dict)\n",
    "\n",
    "citations['partial_citation'] = pd.to_numeric(citations['partial_citation'], errors='coerce')\n",
    "citations.dropna(inplace=True)\n",
    "\n",
    "len(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = citations.groupby('file')\n",
    "\n",
    "labels = {}\n",
    "\n",
    "for name, group in files:\n",
    "    labels[name] = list(set(group['partial_citation']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r_list)\n",
    "\n",
    "r_list = list(r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ffb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "text = {}\n",
    "mlm_labels = {}\n",
    "labels_fin = {}\n",
    "for i in tqdm(range(len(r_list)), desc='preprocessing'):\n",
    "        #print(r_list[i])\n",
    "        with open('opinions{0}'.format(r_list[i]), 'r', encoding='utf-8') as f:\n",
    "            test = json.loads(''.join([x.strip() for x in f.readlines()]))\n",
    "        #print(test['id'])\n",
    "        \n",
    "        to_extract = ''\n",
    "        if test['html_lawbox'] != None:\n",
    "            to_extract = test['html_lawbox']\n",
    "        elif test['html_with_citations'] != None:\n",
    "            to_extract = test['html_with_citations']\n",
    "        else:\n",
    "            to_extract = test['plain_text']\n",
    "\n",
    "        p = preprocess(to_extract)\n",
    "        text[r_list[i]] = p[0]\n",
    "        #mlm_labels[r_list[i]] = p[1]\n",
    "        lab = [0] * 20\n",
    "        if r_list[i] in labels:\n",
    "            for x in labels[r_list[i]]:\n",
    "                lab[int(x)] = 1\n",
    "        labels_fin[r_list[i]] = lab\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[r_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "stopw = set(stopwords.words('english'))\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "to_predict = citations[['title','section', 'subsection']].value_counts()[:100]\n",
    "\n",
    "to_predict = [' '.join(x) for x in list(to_predict.index)]\n",
    "citations['final_citation'] = citations['title'] +' '+ citations['section'] + ' ' + citations['subsection']\n",
    "\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "stats = []\n",
    "for q in tqdm(reversed(to_predict)):\n",
    "    docs = set(citations[citations['final_citation'] == q]['file'])\n",
    "    ngrams = {}\n",
    "    print(q)\n",
    "    for doc in tqdm(docs):\n",
    "        \n",
    "        qu = word_tokenize(re.sub(r'[^a-zA-Z\\s]', '', str(text[doc])))\n",
    "        #print(qu)\n",
    "        ng_temp = [' '.join(x) for x in everygrams(qu, max_len=3)]\n",
    "        \n",
    "        # ng_temp = nlp(sentence)\n",
    "        \n",
    "        for ngram in ng_temp:\n",
    "            count = 0\n",
    "            for word in ngram:\n",
    "                if word.lower() in stopw:\n",
    "                    count += 1\n",
    "                    break\n",
    "            if count == 0:\n",
    "                if ngram in ngrams:\n",
    "                    ngrams[ngram] += 1\n",
    "                else:\n",
    "                    ngrams[ngram] = 1\n",
    "    most_freq = sorted(ngrams.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "    print(most_freq)\n",
    "    stats.append(most_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels_fin), len(mlm_labels), len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15caed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "codes = Counter([x for y in labels_fin for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517bb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a59305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ind = random.sample(range(len(text)), k=len(text))\n",
    "\n",
    "j = int(len(text)*0.8)\n",
    "k = int(len(text)*0.15)\n",
    "l = int(len(text)*0.05)\n",
    "\n",
    "#train_data = [data[i] for i in ind[:j]]\n",
    "train_files = [r_list[i] for i in ind[:j]]\n",
    "\n",
    "val_files = [r_list[i] for i in ind[j:j+k]]\n",
    "\n",
    "test_files = [r_list[i] for i in ind[j+k:j+k+l]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e69a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(files, data, mlm, labels, ds, tokenizer, max_seq_len):\n",
    "    with open('{0}.files'.format(ds), 'w') as f:\n",
    "        f.write('\\n'.join(files))\n",
    "    \n",
    "    for x in tqdm(files):\n",
    "        with open('{0}_{1}'.format(ds, x.replace('/','_')), 'w') as f:\n",
    "            text = {}\n",
    "            text['text'] = tokenizer(data[x], \n",
    "            max_length=max_seq_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt')['input_ids'].tolist()\n",
    "            \n",
    "            \n",
    "            text['mlm'] = tokenizer(mlm[x], \n",
    "            max_length=max_seq_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt')['input_ids'].tolist()\n",
    "            \n",
    "            f.write(json.dumps(text))\n",
    "    \n",
    "            \n",
    "    with open('{0}.label'.format(ds), 'w') as f:\n",
    "        for x in tqdm(files):\n",
    "            f.write(str(labels[x]) + '\\n')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10803f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p roberta\n",
    "from transformers import RobertaTokenizerFast, RobertaForMaskedLM\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "save(train_files, text, mlm_labels, labels_fin, 'roberta/train', tokenizer, 512)\n",
    "save(val_files, text, mlm_labels, labels_fin, 'roberta/val', tokenizer, 512)\n",
    "save(test_files, text, mlm_labels, labels_fin, 'roberta/test', tokenizer, 512)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a93f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(train_label).T.to_json('train_label.json')\n",
    "pd.DataFrame(train_text).T.to_json('train_data.json')\n",
    "\n",
    "pd.DataFrame(val_text).T.to_json('val_data.json')\n",
    "pd.DataFrame(val_label).T.to_json('val_label.json')\n",
    "\n",
    "pd.DataFrame(test_text).T.to_json('test_data.json')\n",
    "pd.DataFrame(test_label).T.to_json('test_label.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir longformer\n",
    "#with open('longformer/train.data', 'w') as f:\n",
    "    #f.write('\\n'.join(train_data))\n",
    "    \n",
    "with open('longformer/train.label', 'w') as f:\n",
    "    f.write('\\n'.join([','.join([str(c) for c in x]) for x in train_label]))\n",
    "\n",
    "#with open('longformer/train.mlm', 'w') as f:\n",
    "    #f.write('\\n'.join(train_mlm))\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    "    f.write(';\\n'.join(['\\'{0}\\''.format(x) for x in train_text]))\n",
    "    \n",
    "\n",
    "#with open('longformer/val.data', 'w') as f:\n",
    "    #f.write('\\n'.join(val_data))\n",
    "    \n",
    "with open('longformer/val.label', 'w') as f:\n",
    "    f.write('\\n'.join([','.join([str(c) for c in x]) for x in val_label]))\n",
    "\n",
    "#with open('longformer/val.mlm', 'w') as f:\n",
    "    #f.write('\\n'.join(val_mlm))\n",
    "\n",
    "with open('val.txt', 'w') as f:\n",
    "    f.write(';\\n'.join(['\\'{0}\\''.format(x) for x in val_text]))\n",
    "    \n",
    "\n",
    "#with open('longformer/test.data', 'w') as f:\n",
    "    #f.write('\\n'.join(test_data))\n",
    "    \n",
    "with open('longformer/test.label', 'w') as f:\n",
    "    f.write('\\n'.join([','.join([str(c) for c in x]) for x in test_label]))\n",
    "\n",
    "#with open('longformer/test.mlm', 'w') as f:\n",
    "    #f.write('\\n'.join(test_mlm))\n",
    "\n",
    "with open('test.txt', 'w') as f:\n",
    "    f.write(';\\n'.join(['\\'{0}\\''.format(x) for x in test_text]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bb241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68fa27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df01f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ceb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a32cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
